---
title: "Getting Started with PITmodelR"
author: Ryan Kinzer and Mike Ackerman
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Getting Started with PITmodelR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, eval = FALSE, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7, fig.height = 4,
  message = TRUE, warning = TRUE
)
```


# Overview

`PITmodelR` streamlines access to [PTAGIS](https://www.ptagis.org/) data through API endpoints and provides helper tools for PIT-tag data workflows, including project discovery, site metadata, tag observations, and survival/migration modeling.

The package does not replace the user's responsibility to ensure the data is used properly and subset to a sample of tags representing their group or population of interest. Additionally, the modeling tools within can lead to erroneous results if careful through isn't given at each step and assumptions are violated.

This vignette demonstrates how to:
* Install the package
* Configure API authentication
* Retrieve mark-recapture-recovery (MRR) project codes and available years of data
* Explore MRR and interrogation site metadata
* Download MRR and observation PIT-tag data
* Retrieve tag event histories

> **Note:** Code chunks below requiring network calls or an API key are set
to `eval = FALSE` to keep this vignette runnable on CRAN. Try them locally
after setting up your API key.

---

# Installation

The `PITmodelR` package can be installed from GitHub using the package `remotes`. Optionally, you can set the argument `build_vignettes = TRUE` to access package
vignettes.

```{r eval = FALSE}
# install.packages("remotes") # if not already, installed

# install PITmodelR (faster, saver version)
remotes::install_github("ryankinzer/PITmodelR")

# to access package vignettes
remotes::install_github("ryankinzer/PITmodelR", build_vignettes = TRUE)
```

As for any R package, it must be loaded into the session environment using the before the functions will operate.

```{r eval = FALSE}
# load PITmodelR package to access functions
library(PITmodelR)
```

If `PITmodelR` was installed using `build_vignettes = TRUE`, vignettes can be access using:

```{r eval = FALSE}
# see available vignettes
browseVignettes(package = "PITmodelR")

# to view this vignette
vignette("getting-started", package = "PITmodelR")

```

---

# Authentication (API key)

Some PTAGIS endpoints require an API key, which is a unique string that identifies you as an authorized user - like a password that allows the `PITmodelR` package to access data on your behalf. Many of the functions in `PITmodelR` require an API key to successfully reach out to these PTAGIS endpoints. You can request a key from PTAGIS staff at this [link](https://www.ptagis.org/Contact). You can pass it directly to functions as a character string using the argument `api_key` (when needed), or set it once for your R session as an environmental variable: 

```{r eval = FALSE}
Sys.setenv(PTAGIS_API_KEY = "YOUR-PTAGIS-API-KEY")
```

Another option is to add `PTAGIS_API_KEY` permanently to your `~/.Renviron` file for persistent use. The steps to do this are not covered in this tutorial. For assistance with adding the API key permanently, contact: mikea@nezperce.org. Remember to **keep your API key private** and do not share it publicly. 

---

# Project Data

PTAGIS data files are organized by projects, each identified by a three-character project code. Previously, these codes were called "Coordinator IDs". Every data file uploaded to PTAGIS has the associated project code appended to its filename.

For more information on project codes, see the [PTAGIS Validation Codes](https://www.ptagis.org/Resources/ValidationCodes) site and navigate to "MRR Project". You can retrieve all project codes that currently have data in PTAGIS using the `get_project_codes()` function:

```{r eval = FALSE}
codes <- get_project_codes()
```

> **Note:** At any time, a help menu for any function can be accessed using e.g., `?get_project_codes`

Users may want to check the years a specific project has submitted data. This task is easily completed using `get_project_years()`:

```{r eval = FALSE}
yrs <- get_project_years(code = "CDR") # "Craig Rabe Projects"
```

Individual mark-recapture-recovery (MRR) files submitted for a given project and year can be obtained with `get_mrr_files()`. The function returns a small data frame containing metadata about the file uploads: 

```{r eval = FALSE}
files <- get_mrr_files(code = "CDR", year = 2024)

# number of files submitted for code and year
nrow(files)

# view names of files submitted
files$name
```

After identifying which files are available, we can retrieve all uploaded data from a specific file using `get_file_data()`. This function returns a list with four objects:

* session – information about the session.
* events - each individual mark, recapture, or recovery recorded during the session.
* session_pdv_fields - session-level *Project Defined Variables (PDVs)*, which are custom fields defined for the session.
* detail_pdv_fields - detail-level PDVs i.e., custom fields associated with events.

For easier analysis, the list can be flattened into a single data frame using `flatten_mrr_file()`. This function replicates session-level fields across all rows and replaces PDV codes with labeled columns, creating a tidy, wide-format table more suitable for further manipulation.

```{r eval = FALSE}
mrr_data <- get_file_data("CDR-2024-067-JCT.xml")

# alternately, using files$name from above
mrr_data <- get_file_data(files$name[1]) # first element in files$name

# if return = "list" (default), view each data frame in mrr_data
mrr_data$session
mrr_data$events
mrr_data$session_pdv_fields
mrr_data$detail_pdv_fields

# flatten mrr_data into single data frame (df)
mrr_df <- flatten_mrr_file(mrr_data)
```

Better yet, you can download data from multiple MRR files in a single step using the batch function `get_batch_file_data()`. Because submitted MRR files may not have standardized PDV fields, the function also contains arguments for how the output is returned based on label consistency. 

`get_batch_file_data()` returns a list with three objects:

- **files** — a nested list with individual objects for each file name supplied to the function. 
- **combined** — a single data frame with the data flattened across all files.  
- **issues** — displays discrepancies between user defined field labels (SPDV and PDV).

> **Note:** User defined fields may contain dates, numbers, or character strings, as such, all data in those fields are returned as a character string. Additional data formatting may be necessary if user defined fields require a specific format.

As an example, let's compile all data submitted by project "CDR" during 2024 from the Secesh River rotary screw trap:

```{r eval = FALSE, message = FALSE}
# return file names containing "SCT
file_names <- files$name[grepl('SCT', files$name)]

# same thing, except using tidyverse
file_names <- files %>%
  filter(str_detect(name, "SCT")) %>%
  pull(name)

# retrieve data for all file_names
all_data <- get_batch_file_data(
  filenames = file_names,      # checks if user defined fields have the same labels, can use "error" to stop on mismatches
  keep_code_cols = FALSE,      # should we keep PDV/SPDV code columns and the user defined labels
  label_conflict = "suffix",   # behavior for file label column collisions
  use_codes_on_conflict = TRUE # prefer consistent code columns across files
)

# --- explore some items in all_data ---
names(all_data$files)                 # list of all individual files
all_data$files$"CDR-2024-101-SCT.xml" # info for a single session
all_data$combined                     # combined mark-recapture-recovery events
all_data$issues                       # label mismatch issues

# summarize a column
table(all_data$combined$event_type)
table(all_data$combined$species_run_rear_type)

# store all_data$combined into a new object
df <- all_data$combined
```

---

# Survival Studies

Survival studies begin with a list of tags (i.e., marks) that represent the group or population of interest and that will later be recaptured or observed. The above functions allow us to download all the data associated with single or multiple MRR files (in this case, mark or tagging files) submitted to PTAGIS. To create a list of tags suitable for a survival study, we first need to clean the MRR data and filter for tags that best represnt our group of interest. This can easily be completed with functions from the `tidyverse` [packages](https://tidyverse.org/).

## Mark List

Creating mark groups requires careful thought and consideration. For instance, you might want to compare survival (and detection probabilities) between parr and smolts from a single location, between smolts between two or more locations, or between parr tagged at a screw trap versus those tagged via electrofishing - which can elucidate differences in tagging mortality or life-history strategies. In other words, consider the research or management question at hand before defining your mark group of interest.

Because these decisions directly affect your analysis and interpretation of results, defining and creating mark groups is a critical first step in any survival study.

As an example, let's create a mark group for wild summer Chinook salmon juveniles tagged and released during fall 2024 from the Secesh River screw trap. We'll accomplish this by filtering some fields in our `df`. 

```{r eval = FALSE}
library(tidyverse)

mark_group <- df %>%
  filter(species_run_rear_type == "12W",                      # wild summer chinook
         migration_year == 2025,                              # emigrating in 2025
         between(release_date, ymd(20240901), ymd(20241231)), # fall trapping period
         release_site == "SECTRP",                            # Secesh RST
         pittag != "..........",                              # only records with pit tags (i.e., exclude tallies)
         !grepl("Recapture", event_type),                     # exclude recapture
         !grepl("Y", conditional_comments))                   # exclude yearlings

# number of tags/fish in mark_group
nrow(mark_group)
```

## Tag Histories

We have our marks - now we need all the observations (i.e., detections) that represent the migration history of each fish. For a single PIT tag, use the `get_tag_history()` function; for multiple tags, use `get_batch_tag_histories()`. Both of these require the user to set `api_key`. The mark and all subsequent observations will eventually be used to create capture histories, which are then used in survival and detection models.

```{r eval = FALSE}
# single tag
tag_history <- get_tag_history(tag_code = "3DD.003E50ED4A")

# alternately, using mark_group$pittag
tag_history <- get_tag_history(tag_code = mark_group$pittag[1])
```

```{r eval = FALSE}
# list of tags
tag_list = mark_group$pittag

# multiple tags
tag_history <- get_batch_tag_histories(tag_codes = tag_list)

# number of unique events
nrow(tag_history)
```

## Observation Locations and Reaches

We next need to define our detection occasions (i.e., locations), and inherently survival reaches, while considering the potential sites that fish may have been observed (mark, observation, recapture, or recovery). **Caution - take some time to think about mark-recapture and survival modeling assumption violations.** To aid us, we can look at a table of all sites that the mark group was observed:

```{r eval = FALSE}
# summary of event types by site
table(tag_history$site_code, tag_history$event_type)
```

In this case, all detection sites look good; however, it is always a good idea to review sites for potential errant observations. Next, we define the occasions (group of sites that we estimate parameters to) and then assign individual sites to each. In this case, we will estimate survivals from SECTRP to ZEN, SFG, and Lower Granite Dam (GRJ and GRS); all locations downstream of LGR can be used to calculate a detection efficiency (and survival) to LGR.

```{r eval = FALSE}
# detection locations (group all sites downstream of LGR)
locs <- c("SECTRP","ZEN","SFG","LGR","Down")

# assign individual detection sites to locations
locs <- list(
  SECTRP = "SECTRP",
  ZEN    = "ZEN",
  SFG    = "SFG",
  LGR    = c("GRJ", "GRS"), # Lower Granite Juvenile Bypass (J) and Spillway (S)
  # upstream -> downstream
  Down   = c(
    "GOJ",                     # Little Goose Dam
    "LMJ",                     # Lower Monumental Dam
    "ICH",                     # Ice Harbor Dam
    "CRESIS",                  # Crescent Island
    "MCJ",                     # McNary Dam
    "JDJ",                     # John Day Dam
    "LMILIS",                  # Little Miller Island
    "B2J", "BCC",              # Bonneville Dam
    "PD7", "PD8", "PDW", "TWX" # Lower Columbia & Estuary
    )
  )
```

Since we have our tag histories and have assigned individual detection sites to study occasions, inherently defining study reaches, we can now build capture histories. This can be accomplished using the `build_mark_histories()` function.

```{r eval = FALSE}
# convert observations to capture histories
ch_list <- build_mark_histories(
  tag_history  = tag_history,
  locs_def     = locs,
  site_col     = "site_code",
  tag_col      = "tag_code",
  time_col     = "event_time",  # optional but helps order ties/revisits
  enforce_order = TRUE,
  keep_unknown  = FALSE
)

# view objects in ch_list
ch_list$ch_data
ch_list$ch_freq
ch_list$mapping
ch_list$dropped_summary
```

Notice that no sites were dropped in `ch_list$droppedsummary`, because we included all detection sites in `locs`. However, you could choose to ignore some detection sites and the `build_mark_histories()` function will drop those observations if `keep_unkown = FALSE`.

```{r eval = FALSE}
# remove a couple sites from locs
locs$Down <- setdiff(locs$Down, c("CRESIS", "LMILIS"))

# retry buld_mark_histories()
ch_list <- build_mark_histories(
  tag_history  = tag_history,
  locs_def     = locs,
  site_col     = "site_code",
  tag_col      = "tag_code",
  time_col     = "event_time",  # optional but helps order ties/revisits
  enforce_order = TRUE,
  keep_unknown  = FALSE
)

# some observations now not included in ch_list$ch_data
ch_list$dropped_summary
```

## CJS Model

We can then feed the capture history data (`$ch_data` tibble) from `build_mark_histories()` into a *Cormack-Jolly-Seber* survival model. To fit the model, we will use the `marked` R package. More information on `marked` can be found at its [journal article](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12065) and [GitHub website](https://github.com/jlaake/marked).

CONTINUE HERE...

```{r eval = FALSE}
# load marked library
library(marked)

# fit CJS model
fit <- fit_marked_cjs(ch_list$ch_data,
                      phi_formula = ~ time,
                      p_formula   = ~ time,
                      hessian = TRUE,
                      conf_level  = 0.95)

# Peek tables
head(fit$phi)
head(fit$p)

# Show plots
if (inherits(fit$plots$phi, "ggplot")) {
  print(fit$plots$phi)
  print(fit$plots$p)
} else if (is.function(fit$plots$phi)) {
  fit$plots$phi()  # base plot
  fit$plots$p()
}

# Full model summary if needed
summary(fit$model)

fit$cum_phi         # cumulative survival with CIs
fit$covariance_mode # "full" if covariance used, otherwise "independence_fallback"

# Plot
if (inherits(fit$plots$cum_phi, "ggplot")) {
  print(fit$plots$cum_phi)
} else {
  fit$plots$cum_phi()
}

res$mapping

```

```{r eval = FALSE}
proc <- process.data(res$ch_data, model = "CJS")
ddl  <- make.design.data(proc)

Phi.time <- list(formula = ~ time)
p.time   <- list(formula = ~ time)

mod <- crm(proc, ddl, model.parameters = list(Phi = Phi.time, p = p.time))
summary(mod)

mod$results
mod$results$reals$Phi

est_preds = predict(mod) %>% 
      map(.f = as_tibble)
```

```{r eval = FALSE}
timing <- summarize_arrival_travel(
  tag_history,
  locs_def = locs,
  site_col = "site_code",
  tag_col  = "tag_code",
  time_col = "event_date",
  tz = "UTC",
  keep_unknown = FALSE
)

# Inspect tables
head(timing$arrivals_long)
head(timing$travel_long)
timing$occasion_summary
timing$leg_summary
timing$dropped_summary  # sites filtered out

# Plots
plots <- plot_arrival_travel(timing)
if (inherits(plots$arrival_ecdf, "ggplot")) {
  print(plots$arrival_ecdf)
  print(plots$travel_time)
} else {
  plots$arrival_ecdf()  # base fallback
  plots$travel_time()
}
```

---

# Interrogation Sites

## List interrogation sites (metadata)

```{r eval = FALSE}
obs_sites <- get_interrogation_sites()
#obs_sites <- get_interrogation_sites(active_only = FALSE)
```

## Get metadata for a single site

```{r eval = FALSE}
get_site_metadata(obs_sites$siteCode[1])
```


## Get site observations (with auto-pagination)

This section is currently non-functional.

```{r eval = FALSE}
obs <- get_site_observations(site_code = "COC", year = 2023)
obs_page1 <- get_site_observations(site_code = "COC", year = 2023, page = 1, page_size = 1000, all_pages = FALSE)
obs_small <- get_site_observations(
  site_code = "COC", year = 2023,
  fields = c("event_time", "tag_code", "antenna", "site_code")
)
```

---

# Error handling & best practices

- Functions use a resilient HTTP layer with retries and helpful error messages.
- When nothing is returned, functions **warn** (instead of stopping) and return an empty tibble or vector.
- Common transformations:
  - Names normalized to `snake_case`
  - Returns standardized to **tibble** for downstream use
- For long queries or production code, consider:
  - Caching responses (e.g., `memoise` package)
  - Rate limiting / backoff on large downloads
  - Filtering by time windows when available

---

# Session info

```{r}
sessionInfo()
```

---

# Appendix: Troubleshooting

- **HTTP 4xx** errors: verify your API key, parameters, and endpoint paths.
- **Empty results**: confirm `site_code`, `tag_code`, date/year filters, and that the site/tag actually has records.
- **Name mismatches**: after `snake_case` normalization, field names may differ slightly from raw API (e.g., `SiteCode` → `site_code`).
